{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Dependencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install datasets wandb pytorch_lightning","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# utils \nimport os\nimport torch\n\n\n# data\nfrom datasets import load_dataset\n\nfrom transformers import BertTokenizerFast, AutoConfig, AutoTokenizer, AutoModel\nfrom transformers import RobertaConfig, RobertaTokenizerFast, RobertaModel\nfrom transformers import XLNetTokenizerFast, XLNetConfig, XLNetForSequenceClassification, XLNetModel\n\n# model\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# training and evaluation\nimport wandb\nimport pytorch_lightning as pl\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom tqdm import tqdm","execution_count":3,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","execution_count":4,"outputs":[{"output_type":"stream","text":"cuda\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Custom Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"class HateSpeechDataset(torch.utils.data.Dataset):\n    \n    def __init__(self, text, label, tokenizer, max_len=200):\n        self.text = text \n        self.label = label\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.label_dict = {\n            \"none\":0,\n            \"racism\":1,\n            \"sexism\":2\n        }\n    \n    def __len__(self):\n        return len(self.text)\n    \n    def __getitem__(self, index):\n        text = self.text[index]\n        label = self.label_dict[self.label[index]]\n        \n        encoding = self.tokenizer.encode_plus(\n            text=text,\n            truncation=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_overflowing_tokens=True,\n            return_attention_mask=True,\n            padding='max_length'\n        )\n        return {\n            \"input_ids\":torch.tensor(encoding['input_ids']).squeeze(),\n            \"attention_mask\":torch.tensor(encoding['attention_mask']).squeeze(),\n            \"label\":torch.tensor([label], dtype=torch.long)\n        }\n        ","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_loader(tokenizer, batch_size, root):\n    \n    train = load_dataset(\"csv\", data_files=root+\"train.csv\")\n    test = load_dataset(\"csv\", data_files=root+\"test.csv\")\n    \n    train_dataset = HateSpeechDataset(text=train['train']['Tweets'], label=train['train']['Label'], tokenizer=tokenizer)\n    test_dataset = HateSpeechDataset(text=test['train']['Tweets'], label=train['train']['Label'], tokenizer=tokenizer)\n    test_dataset, val_dataset = torch.utils.data.random_split(dataset=test_dataset, lengths=[int(len(test_dataset)*0.50), len(test_dataset)-int(len(test_dataset)*0.50)])\n    \n    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, shuffle=True, num_workers=4, batch_size=batch_size)\n    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, shuffle=False, num_workers=4, batch_size=batch_size)\n    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, shuffle=False, num_workers=4, batch_size=batch_size)\n    \n    return train_loader, val_loader, test_loader\n    ","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Models"},{"metadata":{},"cell_type":"markdown","source":"### 1. GRU"},{"metadata":{"trusted":true},"cell_type":"code","source":"class GRUClassfier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, padding_idx, hidden_size=768, num_layers=1, dropout=0.10, num_classes=3):\n        super(GRUClassfier, self).__init__()\n        \n        # embedding layer\n        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=padding_idx)\n        # gru module\n        self.gru = nn.GRU(\n            input_size=embedding_dim,\n            hidden_size=hidden_size,\n            num_layers=num_layers,\n            batch_first=True,\n#             dropout=dropout,\n            bidirectional=True\n        )\n        \n        # full connected layer as classifier\n        self.fc = nn.Sequential(*[\n            nn.Linear(in_features=2*num_layers*hidden_size, out_features=256),\n            nn.ReLU(),\n            nn.Linear(in_features=256, out_features=num_classes)\n        ])\n        \n    def forward(self, x, hidden=None):\n        \n        batch_size = x.shape[0]\n        \n        # get the embedding\n        embedded = self.embedding(x)\n        \n        # pass the embedding and initial hidden states to GRU (cell state will be same as hidden states) \n        _, outputs = self.gru(embedded, hidden)\n        \n        # outputs.shape -> [2*num_layers, batch_size, hidden_size] convert it into batch_first format\n        outputs = outputs.permute(1, 0, 2)\n        outputs = outputs.reshape(batch_size, -1)\n#         print(outputs.shape)\n        \n        # last hidden states of the BidirectionalGRU will be passed to classifier will returns logits \n        logits = self.fc(outputs)\n        return logits","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. RoBERTa "},{"metadata":{"trusted":true},"cell_type":"code","source":"class RobertaClassifier(nn.Module):\n    \n    def __init__(self, model_name, num_classes=3):\n        super(BertClassifier, self).__init__()\n        \n        self.config = RobertaConfig.from_pretrained(pretrained_model_name_or_path=model_name)\n        self.model = RobertaModel.from_pretrained(pretrained_model_name_or_path=model_name, config=self.config)\n        \n        # full connected layer as classifier\n        self.fc = nn.Sequential(*[\n            nn.Linear(in_features=self.config.hidden_size, out_features=256),\n            nn.ReLU(),\n            nn.Linear(in_features=256, out_features=num_classes)\n        ])\n        \n    def forward(self, input_ids, attention_mask=None):\n        _, pooler = self.model(input_ids, attention_mask)\n        logits = self.fc(pooler)\n        return logits\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. BERTTweet"},{"metadata":{"trusted":true},"cell_type":"code","source":"class BertClassifier(nn.Module):\n    \n    def __init__(self, model_name, num_classes=3):\n        super(BertClassifier, self).__init__()\n        \n        self.config = AutoConfig.from_pretrained(pretrained_model_name_or_path=model_name)\n        self.model = AutoModel.from_pretrained(pretrained_model_name_or_path=model_name, config=self.config)\n        \n        # full connected layer as classifier\n        self.fc = nn.Sequential(*[\n            nn.Linear(in_features=self.config.hidden_size, out_features=256),\n            nn.ReLU(),\n            nn.Linear(in_features=256, out_features=num_classes)\n        ])\n        \n    def forward(self, input_ids, attention_mask=None):\n        _, pooler = self.model(input_ids, attention_mask)\n        logits = self.fc(pooler)\n        return logits\n","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 4. XLNet"},{"metadata":{"trusted":true},"cell_type":"code","source":"class XLNetClassifier(nn.Module):\n    \n    def __init__(self, model_name, num_classes=3):\n        super(XLNetClassifier, self).__init__()\n        \n        # xlnet base as feature extractor or  as contextualized embedding layer \n        self.config = XLNetConfig.from_pretrained(pretrained_model_name_or_path=model_name)\n        self.base = XLNetModel.from_pretrained(pretrained_model_name_or_path=model_name, config=self.config)\n        \n        # gru for processing the contextualized embedding into recurrent fashion\n        self.gru = nn.GRU(\n            input_size=self.config.d_model,\n            hidden_size=self.config.d_model,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=True,\n        )\n        \n        # full connected layer as classifier\n        self.fc = nn.Sequential(*[\n            nn.Linear(in_features=2*self.config.d_model, out_features=256),\n            nn.ReLU(),\n            nn.Linear(in_features=256, out_features=num_classes)\n        ])\n        \n    def forward(self, input_ids, attention_mask=None, hidden=None):\n        batch_size = input_ids.shape[0]\n        \n        outputs = self.base(input_ids, attention_mask)\n        _, outputs = self.gru(outputs[0], hidden)\n        \n         # outputs.shape -> [2*num_layers, batch_size, hidden_size] convert it into batch_first format\n        outputs = outputs.permute(1, 0, 2)\n        outputs = outputs.reshape(batch_size, -1)\n        \n        logits = self.fc(outputs)\n        return logits","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training "},{"metadata":{"trusted":true},"cell_type":"code","source":"class LightningModel(pl.LightningModule):\n    \n    def __init__(self, model, config):\n        super(LightningModel, self).__init__()\n        \n        self.model = model\n        self.config = config\n        \n    def forward(self, input_ids, attention_mask):\n        logits  = self.model(input_ids, attention_mask)\n        return logits\n    \n    def configure_optimizers(self):\n        return torch.optim.Adam(params=self.parameters(), lr=config['lr'])\n    \n    def train_dataloader(self):\n        return train_loader\n    \n    def training_step(self, batch, batch_idx):\n        input_ids, attention_mask, targets = batch['input_ids'], batch['attention_mask'], batch['label'].squeeze()\n        logits = self(input_ids, attention_mask)\n        loss = F.cross_entropy(logits, targets)\n        acc = accuracy_score(targets.cpu(), logits.argmax(dim=1).cpu())\n        f1 = f1_score(targets.cpu(), logits.argmax(dim=1).cpu(), average=\"weighted\")\n        wandb.log({\"loss\":loss, \"accuraccy\":acc, \"f1_score\":f1})\n        return {\"loss\":loss, \"accuraccy\":acc, \"f1_score\":f1}\n    \n    def val_dataloader(self):\n        return val_loader\n    \n    def validation_step(self, batch, batch_idx):\n        input_ids, attention_mask, targets = batch['input_ids'], batch['attention_mask'], batch['label'].squeeze()\n        logits = self(input_ids, attention_mask)\n        loss = F.cross_entropy(logits, targets)\n        acc = accuracy_score(targets.cpu(), logits.argmax(dim=1).cpu())\n        f1 = f1_score(targets.cpu(), logits.argmax(dim=1).cpu(), average=\"weighted\")\n        return {\"val_loss\":loss, \"val_accuracy\":torch.tensor([acc]), \"val_f1\":torch.tensor([f1])}\n    \n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        avg_acc = torch.stack([x['val_accuracy'] for x in outputs]).mean()\n        avg_f1 = torch.stack([x['val_f1'] for x in outputs]).mean()\n        wandb.log({\"val_loss\":avg_loss, \"val_accuracy\":avg_acc, \"val_f1\":avg_f1})\n        return {\"val_loss\":avg_loss, \"val_accuracy\":avg_acc, \"val_f1\":avg_f1}\n    \n    def test_dataloader(self):\n        return test_loader\n    \n    def test_step(self, batch, batch_idx):\n        input_ids, attention_mask, targets = batch['input_ids'], batch['attention_mask'], batch['label'].squeeze()\n        logits = self(input_ids, attention_mask)\n        loss = F.cross_entropy(logits, targets)\n        acc = accuracy_score(targets.cpu(), logits.argmax(dim=1).cpu())\n        f1 = f1_score(targets.cpu(), logits.argmax(dim=1).cpu(), average=\"weighted\")\n        precision = precision_score(targets.cpu(), logits.argmax(dim=1).cpu(), average=\"weighted\")\n        recall = recall_score(targets.cpu(), logits.argmax(dim=1).cpu(), average=\"weighted\")\n        return {\"test_loss\":loss, \"test_precision\":torch.tensor([precision]), \"test_recall\":torch.tensor([recall]), \"test_accuracy\":torch.tensor([acc]), \"test_f1\":torch.tensor([f1])}\n    \n    def test_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n        avg_acc = torch.stack([x['test_accuracy'] for x in outputs]).mean()\n        avg_f1 = torch.stack([x['test_f1'] for x in outputs]).mean()\n        avg_precision = torch.stack([x['test_precision'] for x in outputs]).mean()\n        avg_recall = torch.stack([x['test_recall'] for x in outputs]).mean()\n        return {\"test_loss\":avg_loss, \"test_precision\":avg_precision, \"test_recall\":avg_recall, \"test_acc\":avg_acc, \"test_f1\":avg_f1}","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir ../working/models","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"config = {\n    \"root\":\"../input/hatespeechdataset/dataset/\",\n    \"save_dir\":\"../working/models/\",\n    \n    \"project\":\"hate-speech-detection\",\n    \"run_name\":\"xlnet-gru\",\n    \n    \"model_name\":\"xlnet-base-cased\",\n    \"batch_size\":8,\n    \"lr\":1e-5,\n    \n    \"monitor\":\"val_accuracy\",\n    \"min_delta\":0.005,\n    \n    \"filepath\":\"../working/models/{epoch}-{val_accuracy:4f}\",\n    \"precision\":16,\n    \"epochs\":10,\n    \n}","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = XLNetTokenizerFast.from_pretrained(pretrained_model_name_or_path=config[\"model_name\"])\ntrain_loader, val_loader, test_loader = data_loader(tokenizer=tokenizer, batch_size=config[\"batch_size\"], root=config[\"root\"])","execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=798011.0, style=ProgressStyle(descripti…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ada60e0e53ab465daeac11d508c8041f"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1382015.0, style=ProgressStyle(descript…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c7edda70e664658ace5318c5a2fc76d"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=927.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc55720021bb4968bb9d4236e63eff38"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"Using custom data configuration default\n","name":"stderr"},{"output_type":"stream","text":"Downloading and preparing dataset csv/default-d7301296b8e5b97f (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-d7301296b8e5b97f/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4...\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"stream","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-d7301296b8e5b97f/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4. Subsequent calls will reuse this data.\n","name":"stdout"},{"output_type":"stream","text":"Using custom data configuration default\n","name":"stderr"},{"output_type":"stream","text":"Downloading and preparing dataset csv/default-3dc5d768737e23ab (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/csv/default-3dc5d768737e23ab/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4...\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"stream","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-3dc5d768737e23ab/0.0.0/49187751790fa4d820300fd4d0707896e5b941f1a9c644652645b866716a4ac4. Subsequent calls will reuse this data.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"xlnet = XLNetClassifier(model_name=config[\"model_name\"])","execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=760.0, style=ProgressStyle(description_…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94664ecf1a6d41d29e91c5fd5bad1df3"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/configuration_xlnet.py:212: FutureWarning: This config doesn't use attention memories, a core feature of XLNet. Consider setting `mem_len` to a non-zero value, for example `xlnet = XLNetLMHeadModel.from_pretrained('xlnet-base-cased'', mem_len=1024)`, for accurate training performance as well as an order of magnitude faster inference. Starting from version 3.5.0, the default parameter will be 1024, following the implementation in https://arxiv.org/abs/1906.08237\n  FutureWarning,\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, description='Downloading', max=467042463.0, style=ProgressStyle(descri…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e41b8138b7184ed1b550181559b4d189"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LightningModel(model=xlnet, config=config)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logger = WandbLogger(\n    name=config[\"run_name\"],\n    save_dir=config[\"save_dir\"],\n    project=config[\"project\"],\n    log_model=True,\n)\nearly_stopping = EarlyStopping(\n    monitor=config[\"monitor\"],\n    min_delta=config[\"min_delta\"],\n)\ncheckpoints = ModelCheckpoint(\n    filepath=config[\"filepath\"],\n    monitor=config[\"monitor\"],\n    save_top_k=1\n)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer = pl.Trainer(\n    logger=logger,\n    gpus=[0],\n    checkpoint_callback=checkpoints,\n    default_root_dir=\"../working/models/\",\n    max_epochs=config[\"epochs\"],\n    precision=config[\"precision\"]\n)","execution_count":15,"outputs":[{"output_type":"stream","text":"GPU available: True, used: True\nTPU available: False, using: 0 TPU cores\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nUsing native 16bit precision.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.fit(model)","execution_count":16,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","name":"stderr"},{"output_type":"stream","name":"stdout","text":"wandb: Paste an API key from your profile and hit enter: ········\n"},{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.10 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Tracking run with wandb version 0.10.8<br/>\n                Syncing run <strong style=\"color:#cdcd00\">xlnet-gru</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/macab/hate-speech-detection\" target=\"_blank\">https://wandb.ai/macab/hate-speech-detection</a><br/>\n                Run page: <a href=\"https://wandb.ai/macab/hate-speech-detection/runs/ko8s57ln\" target=\"_blank\">https://wandb.ai/macab/hate-speech-detection/runs/ko8s57ln</a><br/>\n                Run data is saved locally in <code>../working/models/wandb/run-20201114_225001-ko8s57ln</code><br/><br/>\n            "},"metadata":{}},{"output_type":"stream","text":"\n  | Name  | Type            | Params\n------------------------------------------\n0 | model | XLNetClassifier | 124 M \n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The validation_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule\n  warnings.warn(*args, **kwargs)\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a49ab1840db4b64b9c9f4b73aa137a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Symlinked 0 file into the W&B run directory, call wandb.save again to sync new files.\n","name":"stderr"},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"1"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.test(model)","execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f6cff2540df487db2e3017cadea4b8e"}},"metadata":{}},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","name":"stderr"},{"output_type":"stream","text":"--------------------------------------------------------------------------------\nDATALOADER:0 TEST RESULTS\n{'test_acc': tensor(0.7488, dtype=torch.float64),\n 'test_f1': tensor(0.8461, dtype=torch.float64),\n 'test_loss': tensor(1.2160, device='cuda:0'),\n 'test_precision': tensor(1., dtype=torch.float64),\n 'test_recall': tensor(0.7488, dtype=torch.float64)}\n--------------------------------------------------------------------------------\n\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The testing_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule\n  warnings.warn(*args, **kwargs)\n","name":"stderr"},{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"[{'test_loss': 1.2159990072250366,\n  'test_precision': 1.0,\n  'test_recall': 0.7487562189054726,\n  'test_acc': 0.7487562189054726,\n  'test_f1': 0.8461492073432374}]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Load from Checkpoint and Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(os.listdir(\"../working/models/\"))","execution_count":18,"outputs":[{"output_type":"stream","text":"['epoch=4-val_accuracy=0.769900.ckpt', 'wandb']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"l  = torch.load(f=\"../working/models/epoch=4-val_accuracy=0.769900.ckpt\")","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_state_dict(l['state_dict'])","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_fn(model, test_loader, val_loader):\n    loss = []\n    acc = []\n    precision = []\n    recall = []\n    f1 = []\n    model.eval()\n    for batch in tqdm(test_loader):\n        input_ids, attention_mask, targets = batch['input_ids'], batch['attention_mask'], batch['label'].squeeze()\n        logits = model(input_ids.to(device), attention_mask.to(device))\n        l = F.cross_entropy(logits.to(device), targets.to(device))\n        a = accuracy_score(targets.cpu(), logits.argmax(dim=1).cpu())\n        f = f1_score(targets.cpu(), logits.argmax(dim=1).cpu(), average=\"weighted\")\n        p = precision_score(targets.cpu(), logits.argmax(dim=1).cpu(), average=\"weighted\")\n        r = recall_score(targets.cpu(), logits.argmax(dim=1).cpu(), average=\"weighted\")\n        loss.append(l.item())\n        acc.append(a)\n        f1.append(f)\n        precision.append(p)\n        recall.append(r)\n    for batch in tqdm(val_loader):\n        input_ids, attention_mask, targets = batch['input_ids'], batch['attention_mask'], batch['label'].squeeze()\n        logits = model(input_ids.to(device), attention_mask.to(device))\n        l = F.cross_entropy(logits.to(device), targets.to(device))\n        a = accuracy_score(targets.cpu(), logits.argmax(dim=1).cpu())\n        f = f1_score(targets.cpu(), logits.argmax(dim=1).cpu(), average=\"weighted\")\n        p = precision_score(targets.cpu(), logits.argmax(dim=1).cpu(), average=\"weighted\")\n        r = recall_score(targets.cpu(), logits.argmax(dim=1).cpu(), average=\"weighted\")\n        loss.append(l.item())\n        acc.append(a)\n        f1.append(f)\n        precision.append(p)\n        recall.append(r)\n        \n    return {\n        \"accuracy\":sum(acc)/len(acc),\n        \"precision\":sum(precision)/len(precision),\n        \"recall\":sum(recall)/len(recall),\n        \"f1\":sum(f1)/len(f1),\n        \"loss\":sum(loss)/len(loss)\n    }\n        ","execution_count":44,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_fn(model, test_loader, val_loader))","execution_count":45,"outputs":[{"output_type":"stream","text":"  0%|          | 0/101 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n100%|██████████| 101/101 [00:11<00:00,  8.53it/s]\n100%|██████████| 101/101 [00:11<00:00,  8.59it/s]","name":"stderr"},{"output_type":"stream","text":"{'accuracy': 0.850763201320132, 'precision': 1.0, 'recall': 0.850763201320132, 'f1': 0.9162689068395092, 'loss': 0.2890760146892897}\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}